{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Linear Regression from Scratch\n",
    "## Predicting Employee Attrition\n",
    "\n",
    "**Objective:** Build a Linear Regression model from scratch (without using sklearn's LinearRegression) to predict employee attrition.\n",
    "\n",
    "- **Target Variable:** Attrition (encoded as 0 = No, 1 = Yes)\n",
    "- **Input Variables (8 features):** Age, DailyRate, DistanceFromHome, MonthlyIncome, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, JobSatisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('ml_dataset.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Number of Rows: {df.shape[0]}\")\n",
    "print(f\"Number of Columns: {df.shape[1]}\")\n",
    "print(f\"\\nColumn Names:\\n{list(df.columns)}\")\n",
    "print(f\"\\nData Types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable: Attrition (Yes=1, No=0)\n",
    "df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(df['Attrition'].value_counts())\n",
    "print(f\"\\nAttrition Rate: {df['Attrition'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 8 input features and 1 output variable\n",
    "input_features = [\n",
    "    'Age',\n",
    "    'DailyRate',\n",
    "    'DistanceFromHome',\n",
    "    'MonthlyIncome',\n",
    "    'TotalWorkingYears',\n",
    "    'YearsAtCompany',\n",
    "    'YearsInCurrentRole',\n",
    "    'JobSatisfaction'\n",
    "]\n",
    "\n",
    "target = 'Attrition'\n",
    "\n",
    "print(f\"Input Features ({len(input_features)}): {input_features}\")\n",
    "print(f\"Target Variable: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix X and target vector y\n",
    "X = df[input_features].values\n",
    "y = df[target].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nStatistical Summary of Features:\")\n",
    "df[input_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (Min-Max Normalization) - from scratch\n",
    "def min_max_normalize(X):\n",
    "    \"\"\"Normalize features using Min-Max scaling.\"\"\"\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_normalized = (X - X_min) / (X_max - X_min + 1e-8)  # small epsilon to avoid division by zero\n",
    "    return X_normalized, X_min, X_max\n",
    "\n",
    "X_normalized, X_min, X_max = min_max_normalize(X)\n",
    "print(\"Feature scaling completed (Min-Max Normalization).\")\n",
    "print(f\"Normalized feature matrix shape: {X_normalized.shape}\")\n",
    "print(f\"\\nMin of normalized features: {X_normalized.min(axis=0)}\")\n",
    "print(f\"Max of normalized features: {X_normalized.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split from scratch (80% train, 20% test)\n",
    "def train_test_split_scratch(X, y, test_size=0.2, random_seed=42):\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    n_samples = X.shape[0]\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_scratch(X_normalized, y, test_size=0.2, random_seed=42)\n",
    "\n",
    "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Testing set:  X_test={X_test.shape}, y_test={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for selected features\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[input_features + [target]].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap - Selected Features vs Attrition', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar chart\n",
    "attrition_counts = df['Attrition'].value_counts()\n",
    "axes[0].bar(['No (0)', 'Yes (1)'], [attrition_counts[0], attrition_counts[1]], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Attrition Distribution', fontsize=14)\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate([attrition_counts[0], attrition_counts[1]]):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie([attrition_counts[0], attrition_counts[1]], labels=['No', 'Yes'],\n",
    "            autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "axes[1].set_title('Attrition Percentage', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(input_features):\n",
    "    axes[i].hist(df[feature], bins=20, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(feature, fontsize=11)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Distribution of Input Features', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Regression from Scratch\n",
    "\n",
    "### Model: $\\hat{y} = X \\cdot W + b$\n",
    "\n",
    "### Cost Function: Mean Squared Error (MSE)\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Gradient Descent Update Rules:\n",
    "$$W = W - \\alpha \\cdot \\frac{\\partial MSE}{\\partial W}$$\n",
    "$$b = b - \\alpha \\cdot \\frac{\\partial MSE}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implemented from scratch using Gradient Descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def _compute_cost(self, y_true, y_pred):\n",
    "        \"\"\"Compute Mean Squared Error.\"\"\"\n",
    "        n = len(y_true)\n",
    "        cost = (1 / (2 * n)) * np.sum((y_true - y_pred) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Linear Regression model using Gradient Descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "        y : numpy array of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias to zeros\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Gradient Descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass: compute predictions\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))  # gradient w.r.t weights\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)          # gradient w.r.t bias\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Record cost\n",
    "            cost = self._compute_cost(y, y_pred)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Print progress every 100 iterations\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Iteration {i+1}/{self.n_iterations} - Cost: {cost:.6f}\")\n",
    "        \n",
    "        print(f\"\\nTraining Complete!\")\n",
    "        print(f\"Final Cost: {self.cost_history[-1]:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return model parameters.\"\"\"\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'bias': self.bias,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'n_iterations': self.n_iterations\n",
    "        }\n",
    "\n",
    "print(\"LinearRegressionScratch class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = LinearRegressionScratch(learning_rate=0.1, n_iterations=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost function over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(model.cost_history) + 1), model.cost_history, color='#e74c3c', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Cost (MSE)', fontsize=12)\n",
    "plt.title('Cost Function Convergence During Training', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model parameters\n",
    "params = model.get_params()\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"Bias (Intercept): {params['bias']:.6f}\")\n",
    "print(f\"\\nWeights (Coefficients):\")\n",
    "for feature, weight in zip(input_features, params['weights']):\n",
    "    print(f\"  {feature:25s}: {weight:+.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics from scratch\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error.\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    \"\"\"Calculate R-squared (Coefficient of Determination).\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)    # Residual Sum of Squares\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)  # Total Sum of Squares\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"Evaluation metric functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on training and testing sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Training Set Metrics\n",
    "print(\"=\" * 50)\n",
    "print(\"       TRAINING SET PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Squared Error (MSE):     {mean_squared_error(y_train, y_train_pred):.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {root_mean_squared_error(y_train, y_train_pred):.6f}\")\n",
    "print(f\"Mean Absolute Error (MAE):    {mean_absolute_error(y_train, y_train_pred):.6f}\")\n",
    "print(f\"R-squared (R²):              {r_squared(y_train, y_train_pred):.6f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Testing Set Metrics\n",
    "print(\"=\" * 50)\n",
    "print(\"       TESTING SET PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Squared Error (MSE):     {mean_squared_error(y_test, y_test_pred):.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {root_mean_squared_error(y_test, y_test_pred):.6f}\")\n",
    "print(f\"Mean Absolute Error (MAE):    {mean_absolute_error(y_test, y_test_pred):.6f}\")\n",
    "print(f\"R-squared (R²):              {r_squared(y_test, y_test_pred):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted Values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='#3498db', edgecolors='black', linewidth=0.5)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', linewidth=2)\n",
    "axes[0].set_xlabel('Actual Values', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_title(f'Training Set: Actual vs Predicted\\nR² = {r_squared(y_train, y_train_pred):.4f}', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Testing set\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.5, color='#e74c3c', edgecolors='black', linewidth=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "axes[1].set_xlabel('Actual Values', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_title(f'Testing Set: Actual vs Predicted\\nR² = {r_squared(y_test, y_test_pred):.4f}', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(y_test_pred, residuals_test, alpha=0.5, color='#9b59b6', edgecolors='black', linewidth=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0].set_title('Residual Plot (Test Set)', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals_test, bins=30, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of Residuals (Test Set)', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (based on absolute weight values)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': input_features,\n",
    "    'Weight': model.weights,\n",
    "    'Absolute Weight': np.abs(model.weights)\n",
    "}).sort_values('Absolute Weight', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#e74c3c' if w < 0 else '#2ecc71' for w in feature_importance['Weight']]\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Absolute Weight'], color=colors, edgecolor='black')\n",
    "plt.xlabel('Absolute Weight', fontsize=12)\n",
    "plt.title('Feature Importance (Based on Model Weights)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Table:\")\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Classification Analysis\n",
    "\n",
    "Since Attrition is a binary variable (0 or 1), we can threshold the linear regression output at 0.5 to make binary predictions and evaluate classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert continuous predictions to binary (threshold = 0.5)\n",
    "y_test_binary = (y_test_pred >= 0.5).astype(int)\n",
    "y_train_binary = (y_train_pred >= 0.5).astype(int)\n",
    "\n",
    "# Accuracy from scratch\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Confusion Matrix from scratch\n",
    "def confusion_matrix_scratch(y_true, y_pred):\n",
    "    \"\"\"Calculate confusion matrix.\"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))  # True Positive\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))  # True Negative\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))  # False Positive\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))  # False Negative\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_binary)\n",
    "test_accuracy = accuracy_score(y_test, y_test_binary)\n",
    "cm = confusion_matrix_scratch(y_test, y_test_binary)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"   CLASSIFICATION PERFORMANCE (Threshold=0.5)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy*100:.2f}%\")\n",
    "print(f\"\\nConfusion Matrix (Test Set):\")\n",
    "print(f\"  TN={cm[0,0]:4d}  FP={cm[0,1]:4d}\")\n",
    "print(f\"  FN={cm[1,0]:4d}  TP={cm[1,1]:4d}\")\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision = cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0\n",
    "recall = cm[1,1] / (cm[1,1] + cm[1,0]) if (cm[1,1] + cm[1,0]) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No (0)', 'Yes (1)'],\n",
    "            yticklabels=['No (0)', 'Yes (1)'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix (Test Set)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### Summary:\n",
    "- Built a **Linear Regression model from scratch** using Gradient Descent (no sklearn)\n",
    "- **Target Variable:** Attrition (encoded: Yes=1, No=0)\n",
    "- **8 Input Features:** Age, DailyRate, DistanceFromHome, MonthlyIncome, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, JobSatisfaction\n",
    "- Applied **Min-Max Normalization** for feature scaling\n",
    "- Split data into **80% training / 20% testing**\n",
    "- Evaluated using **MSE, RMSE, MAE, R²** and classification metrics (accuracy, precision, recall, F1)\n",
    "\n",
    "### Key Observations:\n",
    "- Linear Regression is not ideal for binary classification (Attrition is Yes/No), but it provides useful insights into feature relationships\n",
    "- The model weights indicate which features have positive or negative associations with attrition\n",
    "- For better classification performance, Logistic Regression or other classifiers would be more appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"   Assignment 1 - Linear Regression from Scratch\")\n",
    "print(\"   Status: COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
